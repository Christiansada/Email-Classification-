{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bVECsQ7R6sGM"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import string\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "data = pd.read_csv(\"SPAM text message 20170820 - Data.csv\")\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = ''.join([char for char in text if char not in string.punctuation])\n",
        "    return text\n",
        "\n",
        "# Apply the preprocessing function to the 'Message' column\n",
        "data['Message'] = data['Message'].apply(preprocess_text)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Text Tokenization and Padding Sequences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(data['Message'])\n",
        "sequences = tokenizer.texts_to_sequences(data['Message'])\n",
        "padded_sequences = pad_sequences(sequences, padding='post')\n",
        "\n",
        "# Prepare the Labels\n",
        "labels = np.array(data['Category'].map({'ham': 0, 'spam': 1}))\n",
        "\n",
        "# Split the Data into Training and Testing Sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build the CNN Model\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "embedding_dim = 16\n",
        "max_length = len(max(padded_sequences, key=len))\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    Conv1D(64, 5, activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the Model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Print the Model Summary (Architecture)\n",
        "model.summary()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSEMk9k16-O6",
        "outputId": "076daa5b-e4b3-49c8-bd1f-2049b44c4602"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 171, 16)           154528    \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 167, 64)           5184      \n",
            "                                                                 \n",
            " global_max_pooling1d (Globa  (None, 64)               0         \n",
            " lMaxPooling1D)                                                  \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 159,777\n",
            "Trainable params: 159,777\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the Model\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=1)\n",
        "\n",
        "# Evaluate the Model on the Testing Data\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
        "\n",
        "# Print the Test Accuracy\n",
        "print(\"\\nTest Accuracy: {:.4f}\".format(test_accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwpp_0UF7B_1",
        "outputId": "b857a150-776c-4199-f4e7-38169ca80d18"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "112/112 [==============================] - 3s 16ms/step - loss: 0.4363 - accuracy: 0.8673 - val_loss: 0.3767 - val_accuracy: 0.8576\n",
            "Epoch 2/10\n",
            "112/112 [==============================] - 2s 20ms/step - loss: 0.2258 - accuracy: 0.9083 - val_loss: 0.1154 - val_accuracy: 0.9608\n",
            "Epoch 3/10\n",
            "112/112 [==============================] - 2s 18ms/step - loss: 0.0524 - accuracy: 0.9871 - val_loss: 0.0777 - val_accuracy: 0.9765\n",
            "Epoch 4/10\n",
            "112/112 [==============================] - 2s 14ms/step - loss: 0.0219 - accuracy: 0.9952 - val_loss: 0.0683 - val_accuracy: 0.9843\n",
            "Epoch 5/10\n",
            "112/112 [==============================] - 1s 13ms/step - loss: 0.0114 - accuracy: 0.9975 - val_loss: 0.0692 - val_accuracy: 0.9854\n",
            "Epoch 6/10\n",
            "112/112 [==============================] - 1s 13ms/step - loss: 0.0062 - accuracy: 0.9992 - val_loss: 0.0688 - val_accuracy: 0.9832\n",
            "Epoch 7/10\n",
            "112/112 [==============================] - 1s 13ms/step - loss: 0.0038 - accuracy: 0.9994 - val_loss: 0.0706 - val_accuracy: 0.9832\n",
            "Epoch 8/10\n",
            "112/112 [==============================] - 1s 13ms/step - loss: 0.0027 - accuracy: 0.9997 - val_loss: 0.0724 - val_accuracy: 0.9832\n",
            "Epoch 9/10\n",
            "112/112 [==============================] - 2s 15ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 0.0737 - val_accuracy: 0.9832\n",
            "Epoch 10/10\n",
            "112/112 [==============================] - 3s 22ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.0748 - val_accuracy: 0.9832\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0566 - accuracy: 0.9830\n",
            "\n",
            "Test Accuracy: 0.9830\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qdQG_IDHxBoE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}